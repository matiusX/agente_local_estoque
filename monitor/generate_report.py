"""Gera relat√≥rio quinzenal de progresso de um planejamento de estoque
usando um snapshot JSON estruturado + OpenAI LLM.

Requisitos:
- python-dotenv (para OPENAI_API_KEY) ou defina OPENAI_API_KEY em env.
- openai>=1.3.7 (nova lib)

Uso:
$ python generate_relatorio_quinzenal.py structured_analysis_eg.json  > relatorio_Q2.md
"""

from __future__ import annotations
import json, sys, textwrap, os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

from openai import OpenAI, RateLimitError  # pip install openai>=1.3.7

MODEL_NAME = "gpt-4o"
TEMPERATURE = 0.2
MAX_TOKENS = 2400  # suficiente p/ ~3k palavras

# ---------------------------------------------------------------------------
#  Utilidades
# ---------------------------------------------------------------------------

def load_snapshot(path: str | Path) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as fp:
        return json.load(fp)


def iso_to_pt(date_iso: str) -> str:
    return datetime.fromisoformat(date_iso).strftime("%d/%m/%Y")


# ---------------------------------------------------------------------------
#  Prompt Engineering
# ---------------------------------------------------------------------------

SYSTEM_PROMPT = textwrap.dedent(
    """
    Voc√™ √© um Analista S√™nior de Planejamento de Estoques.
    Objetivo: produzir um **relat√≥rio quinzenal** claro, conciso e orientado a a√ß√£o, usando os dados estruturados do snapshot JSON.
    ‚Ä¢ Sempre responda em **Markdown** bem formatado.
    ‚Ä¢ Apresente n√∫meros com separador decimal v√≠rgula e unidade, quando existir.
    ‚Ä¢ Realce alertas cr√≠ticos com emoji üî¥ e conquistas com ‚úÖ.
    ‚Ä¢ Limite o **Resumo Executivo** a no m√°ximo 200 palavras.
    ‚Ä¢ Assuma racioc√≠nio anal√≠tico antes de escrever.
    """
)

HUMAN_PROMPT_TEMPLATE = textwrap.dedent(
    """
    ## CONTEXTO JSON
    ```json
    {context_json}
    ```

    ## TAREFA
    Gere o relat√≥rio quinzenal para o contratante **ID {contratante_id}**, cobrindo o per√≠odo {periodo_str}.

    Estrutura esperada:
    1. **Resumo Executivo** (‚â§ 200 palavras)
    2. **Painel de M√©tricas-Objetivo**
       - Tabela: M√©trica | Baseline | Atual | Meta | Œî% | Status
    3. **Tend√™ncias & Insights**
       - Destaque m√©tricas observacionais relevantes (sem meta) com suas tend√™ncias.
    4. **Avalia√ß√£o de A√ß√µes**
       - Tabela: A√ß√£o | Implementada | Efic√°cia | M√©tricas afetadas | Pr√≥ximos Passos
    5. **Recomenda√ß√µes** (curto prazo ‚â§2 semanas & m√©dio prazo)
       - Inclua matriz ICE (Impacto, Confian√ßa, Esfor√ßo) 1-10-10.
    6. **Pr√≥ximos Passos**

    Gere respostas num portugu√™s formal, mas direto.
    """
)


def build_human_prompt(snapshot: Dict[str, Any]) -> str:
    """Gera o prompt de usu√°rio com o JSON compacto (truncando m√©tricas se >15)."""
    # Seleciona at√© 15 m√©tricas (priorizando com target e alerts)
    metrics = snapshot["metrics"]
    target_metrics = [m for m in metrics if metrics[m].get("has_target")]
    observe_metrics = [m for m in metrics if not metrics[m].get("has_target")]
    selected = (
        target_metrics[:10]
        + observe_metrics[:5]
    )
    compact_metrics = {k: metrics[k] for k in selected}
    # substitui no contexto para ficar leve
    ctx = snapshot.copy()
    ctx["metrics"] = compact_metrics
    ctx_json = json.dumps(ctx, ensure_ascii=False, indent=2)[:6000]  # cabe no contexto

    periodo = f"{iso_to_pt(ctx['window']['start'])} ‚Äì {iso_to_pt(ctx['window']['end'])}" if "window" in ctx else "(√∫ltimas 2 semanas)"

    return HUMAN_PROMPT_TEMPLATE.format(
        context_json=ctx_json,
        contratante_id=ctx.get("contratante_id", "?"),
        periodo_str=periodo,
    )


# ---------------------------------------------------------------------------
#  Gera√ß√£o de relat√≥rio
# ---------------------------------------------------------------------------

def generate_report(snapshot_path: str | Path) -> str:
    snapshot = load_snapshot(snapshot_path)
    human_prompt = build_human_prompt(snapshot)

    client = OpenAI()

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            temperature=TEMPERATURE,
            max_tokens=MAX_TOKENS,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": human_prompt},
            ],
        )
        return response.choices[0].message.content.strip()
    except RateLimitError as e:
        raise SystemExit(f"Chamada √† API excedeu limite: {e}")


# ---------------------------------------------------------------------------
#  CLI
# ---------------------------------------------------------------------------

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Uso: python generate_relatorio_quinzenal.py <snapshot.json> [saida.md]", file=sys.stderr)
        sys.exit(1)

    in_path = Path(sys.argv[1])
    out_path = Path(sys.argv[2]) if len(sys.argv) > 2 else None

    report_md = generate_report(in_path)

    if out_path:
        out_path.write_text(report_md, encoding="utf-8")
        print(f"Relat√≥rio gravado em {out_path}")
    else:
        print(report_md)
